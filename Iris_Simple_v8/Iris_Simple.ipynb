{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Iris Classification Challenge\n",
    "\n",
    "#### Based on a notebook by [Randal S. Olson](http://www.randalolson.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Challenge, you will work through a simple machine learning example from start to finish to see what a typical data analysis pipeline looks like. In addition to providing code examples, we will also discuss good practices for data anaylsis. Note that the code of the notebook runs in your browser, allowing you to interact with it and immediately see the outcomes.\n",
    "\n",
    "The data used here is a slightly modified version of the [Iris data set](https://archive.ics.uci.edu/ml/datasets/Iris), a famous data set collected by botanist Edward Anderson and popularized by the statistician Ronald Fisher in his classic 1936 paper, \"The use of multiple measurements in taxonomic problems\". Anderson measured the properties of three different species of iris flowers. The data set contains four features from each sample: the length and width of the petals and sepals, in centimeters. \n",
    "\n",
    "<img src=\"images/iris_petal_sepal.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "0. [Introduction](#Introduction)\n",
    "\n",
    "1. [Step 1: Framing the problem](#Step-1:-Framing-the-problem) \n",
    "\n",
    "2. [Step 2: Checking the data](#Step-2:-Checking-the-data)\n",
    "\n",
    "3. [Step 3: Tidying the data](#Step-3:-Tidying-the-data)\n",
    "\n",
    "4. [Step 4: Exploratory analysis](#Step-4:-Exploratory-analysis)\n",
    "\n",
    "5. [Step 5: Classification](#Step-5:-Classification)\n",
    "\n",
    "6. [Step 6: Evaluation with cross-validation](#Step-6:-Evaluation-with-cross-validation)\n",
    "\n",
    "7. [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "For the purposes of this exercise, let's pretend we would like to create a smartphone app that automatically identifies species of flowers from pictures taken on the smartphone. We're working with a team of data scientists and will be building a part of the data analysis pipeline for this app.\n",
    "\n",
    "Our task is to create a demo machine learning model that **takes four measurements from flowers** (petal length, petal width, sepal length, and sepal width) and **identifies the species based on these measurements** alone. The measurements we're using currently come from hand-measurements by our field researchers, but they will be automated using an image processing model in the future.\n",
    "\n",
    "To develop the demo, we've been given a [data set](iris-data.csv) which includes measurements for three types of *Iris* flowers: Iris versicolor, Iris setosa, and Iris virginica.\n",
    "\n",
    "<img src=\"images/iris-types.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Framing the problem \n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The first step to any data analysis project is to define the question or problem we're trying to solve, and to define a measure (or set of measures) for our success at solving that task. The following set of questions can serve as a useful guide:\n",
    "\n",
    ">What kind of data analytic question are we trying to answer (e.g. exploration, association causality)?\n",
    "\n",
    "We're trying to classify the species (i.e., class) of Iris flowers based on four measurements: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    ">What is the metric for success?\n",
    "\n",
    "Since we're performing classification, we can use accuracy — the fraction of correctly classified flowers — to quantify how well our model is performing. We aim to achieve at least 90% accuracy.\n",
    "\n",
    ">What is the context for the question and the scientific or business application?\n",
    "\n",
    "We're building a part of the data analysis pipeline for a smartphone app that will be able to classify species of flowers from pictures taken on the smartphone. In the future, this pipeline will be connected to another pipeline that automatically measures from pictures the traits we're using to perform this classification.\n",
    "\n",
    ">What is the experimental design?\n",
    "\n",
    "We know that our field researchers have hand-measured 50 randomly-sampled flowers of each species using a standardized methodology. The field researchers take pictures of each flower they sample from pre-defined angles so the measurements and species can be confirmed by the other field researchers at a later point. \n",
    "\n",
    ">Can the question be answered with the available data?\n",
    "\n",
    "The data set we currently have is only for three types of *Iris* flowers. The model trained on this data set will thus also only work for Iris flowers, and we will need more data to create a general flower classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Checking the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "The next step is to look closer at the data we're working with. Spotting potential errors in the data set early can save a lot of time during our later analysis. \n",
    "\n",
    "Let's start by reading the data into a pandas DataFrame object and displaying the first five rows. A DataFrame object is a 2-dimensional, labeled data structure with columns that can be of different types; you can think of it like of a spreadsheet.\n",
    "\n",
    "**Remember to press *Shift+Enter* to run each code cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first line below loads the pandas package, which provides the DataFrame structure; \n",
    "# an overview of the packages used in this notebook is provided in the last section\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from file\n",
    "iris_data = pd.read_csv('iris-data.csv')\n",
    "\n",
    "# Display the first five rows\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row in the data file defines the column headers, which indicate which measurement each column represents and the unit of measurement. Each row below the header row represents an entry for a flower: four measurements and one class, which tells us the species of the flower.\n",
    "\n",
    "One of the first things we should look for is **missing data.** Thankfully, the field researchers already told us that they put a 'NA' into the spreadsheet when they were missing a measurement. We can tell pandas to automatically interpret values with 'NA' as missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv('iris-data.csv', na_values=['NA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's always a good idea to look at the distribution of the data — especially to identify potential **outliers**. Let's start by printing out some summary statistics about the numerical attributes of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get some useful pieces of information from this table. For example, we observe that five `petal_width_cm` entries are missing. However, identifying outliers and errors in a large table of numbers is difficult; some visualization of the data would be helpful. \n",
    "\n",
    "For this, let's set up the notebook so we can plot inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line tells the notebook to show plots inside of the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way to obtain an overview of the data is to plot **histograms** for the numerical attributes. A histogram shows the number of data points (on the vertical axis) for which a selected attribute falls in a given value range (on the horizontal axis). Here is the histogram of the petal length attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data['petal_length_cm'].hist()\n",
    "plt.xlabel('petal_length_cm')\n",
    "plt.ylabel('number of samples')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful way to visualize data are **scatterplots**. A scatterplot displays the relationship between two numerical attributes by showing the data as a collection of points; the x- and y-coordinate of each point are the values of the two selected attributes for that point. In the following plot, we use petal length and petal width as attributes, and additionaly color each point to indicate the class it belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(x=\"petal_length_cm\", y=\"petal_width_cm\",data=iris_data,hue='class')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>**It's your turn**</font> \n",
    "\n",
    "Pick two attributes and plot your own scatterplot!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's combine histograms and scatterplots to create a **scatterplot matrix**. Scatterplot matrices plot the histograms of each column along the diagonal, and a matrix of scatterplots for the combination of any two attributes. They make for an efficient tool to look for errors in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We temporarily drop the rows with 'NA' values before plotting\n",
    "# because the plotting function does not know how to handle them\n",
    "sb.pairplot(iris_data.dropna())\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot matrix becomes even more informative if we again use colors to indicate the classes, allowing us see some further issues with the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(iris_data.dropna(),hue='class')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>**It's your turn**</font> \n",
    "\n",
    "Take a moment to analyse the charts: Which potential issues do you notice? Write them down in the cell below before moving on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ...\n",
    "- ... \n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tidying the data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "So far, we could observe several issues with our data set:\n",
    "\n",
    "1. There are five classes when there should only be three, likely indicating some labeling errors.\n",
    "\n",
    "2. There are some clear outliers in the measurements that may be erroneous: one `sepal_width_cm` entry for `Iris-setosa` falls well outside its normal range; and several `sepal_length_cm` entries for `Iris-versicolor` are near-zero for some reason.\n",
    "\n",
    "3. We have some rows with missing values.\n",
    "\n",
    "In all these cases, we need to figure out what to do with the erroneous data. Let's walk through the issues one by one and fix them before proceeding with the analysis. \n",
    "\n",
    "#### 1. There are five classes when there should only be three.\n",
    "\n",
    "After talking with the field researchers, it sounds like one of them forgot to add `Iris-` before their `Iris-versicolor` entries. The other extraneous class, `Iris-setossa`, was simply a typo that they forgot to fix.\n",
    "\n",
    "We can easily fix these errors in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the mislabeled classes\n",
    "iris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n",
    "iris_data.loc[iris_data['class'] == 'Iris-setossa', 'class'] = 'Iris-setosa'\n",
    "\n",
    "# Checking that only the correct three class types remain\n",
    "iris_data['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a There are some clear outliers that may be erroneous: one `sepal_width_cm` entry for `Iris-setosa` well outside its normal range.\n",
    "\n",
    "Fixing outliers can be tricky, because it's rarely clear whether the outlier was caused by measurement error, recording the data in improper units, or if the outlier is a real anomaly. If we decide to exclude any data, we need to make sure to document what data we excluded and provide solid reasoning for excluding that data. \n",
    "\n",
    "In the case of the one anomalous `sepal_width_cm` entry for `Iris-setosa`, let's say our field researchers know that it's impossible for `Iris-setosa` to have a sepal width below 2.5 cm. We therefore decide to remove that specific entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep only 'Iris-setosa' rows for which the sepal width is equal or greater than 2.5 cm\n",
    "iris_data = iris_data.loc[(iris_data['class'] != 'Iris-setosa') | (iris_data['sepal_width_cm'] >= 2.5)]\n",
    "\n",
    "iris_data.loc[iris_data['class'] == 'Iris-setosa', 'sepal_width_cm'].hist()\n",
    "plt.xlabel('sepal_width_cm for Iris-setosa')\n",
    "plt.ylabel('number of samples')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b There are some clear outliers that may be erroneous: several `sepal_length_cm` entries for `Iris-versicolor` are near-zero.\n",
    "\n",
    "Let's display the rows containing the suspect entries and take a look at the distribution of sepal lengths for `Iris-versicolor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram\n",
    "iris_data.loc[iris_data['class'] == 'Iris-versicolor', 'sepal_length_cm'].hist()\n",
    "plt.xlabel('sepal_length_cm for Iris-versicolor')\n",
    "plt.ylabel('number of samples')\n",
    "\n",
    "\n",
    "# Display the Iris-versicolor rows with a sepal length below 1 cm\n",
    "iris_data.loc[(iris_data['class'] == 'Iris-versicolor') &\n",
    "              (iris_data['sepal_length_cm'] < 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These near-zero `sepal_length_cm` entries seem to be off by two orders of magnitude, as if they had been recorded in meters instead of centimeters. After some brief correspondence with the field researchers, we find that one of them forgot to convert those measurements to centimeters. Let's do that for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data.loc[(iris_data['class'] == 'Iris-versicolor') &\n",
    "              (iris_data['sepal_length_cm'] < 1.0),\n",
    "              'sepal_length_cm'] *= 100.0\n",
    "\n",
    "# Checking properties of the updated sepal_length_cm attribute for Iris versicolor\n",
    "iris_data.loc[(iris_data['class'] == 'Iris-versicolor'),'sepal_length_cm'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. We have some rows with missing values.\n",
    "\n",
    "Let's take a look at these rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows which contain missing values\n",
    "iris_data.loc[(iris_data['sepal_length_cm'].isnull()) |\n",
    "              (iris_data['sepal_width_cm'].isnull()) |\n",
    "              (iris_data['petal_length_cm'].isnull()) |\n",
    "              (iris_data['petal_width_cm'].isnull())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would not be ideal to exclude these rows considering they all belong to the `Iris-setosa` class. Since it seems like the missing data is systematic — all of the missing values are in the same column for the same *Iris* type — this error could potentially bias our analysis.\n",
    "\n",
    "One way to deal with missing data is **mean imputation**: If we know that the values for a measurement fall in a certain range, we can fill in empty values with the average of that measurement. Besides the mean, using the median or zero can be reasonable choices. \n",
    "\n",
    "Let's use the mean here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics of the petal_width_cm attribute for the Iris-setosa class\n",
    "iris_data.loc[(iris_data['class'] == 'Iris-setosa'),'petal_width_cm'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the petal widths for `Iris-setosa` fall within the 0.2-0.3 range. Let's fill in the missing entries with the average measured petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_petal_width = iris_data.loc[iris_data['class'] == 'Iris-setosa', 'petal_width_cm'].mean()\n",
    "\n",
    "# Fill missing Iris setosa petal width values with the mean value\n",
    "iris_data.loc[(iris_data['class'] == 'Iris-setosa') &\n",
    "              (iris_data['petal_width_cm'].isnull()),\n",
    "              'petal_width_cm'] = average_petal_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Sometimes mean imputation might not be the best choice to deal with missing data, and we might want to remove incomplete rows. In such a case, you can drop all rows with missing data with the `dropna()` call:\n",
    "\n",
    "    iris_data.dropna(inplace=True)\n",
    "\n",
    "Now that we've cleaned the data, we don't want to repeat this process every time we work with the data set. Let's save the tidied data file *as a separate file* and work directly with that data file from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write cleaned data set to new csv (comma-separated values) file\n",
    "iris_data.to_csv('iris-data-clean.csv', index=False)\n",
    "\n",
    "# Load cleaned data into new DataFrame object\n",
    "iris_data_clean = pd.read_csv('iris-data-clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go on, let's summarize some general takeaways related to tidying the data:\n",
    "\n",
    "* Make sure the data is labeled properly\n",
    "* Make sure the data falls within the expected range, and use domain knowledge whenever possible to define that expected range\n",
    "* Deal with missing data: replace it if you can; otherwise drop it\n",
    "* Never tidy the data manually because that is not easily reproducible; instead, use code as a record of how you modified the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory analysis\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Exploratory analysis is the step where we start delving deeper into the data set beyond the outliers and errors. We'll be looking to answer questions such as:\n",
    "\n",
    "* How is my data distributed?\n",
    "* Are there any correlations in my data?\n",
    "* Are there any confounding factors that explain these correlations?\n",
    "\n",
    "This is the stage where we plot the data in as many ways as possible. Let's return to the scatterplot matrix that we used earlier and take a look at the cleaned data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(iris_data_clean)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is normally distributed for the most part, which is great news if we plan on using any modeling methods that assume normally distributed data. But there's something strange going on with the petal measurements, which we suspect is related to the different `Iris` species. Let's color code the data by the class again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(iris_data_clean, hue='class')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the strange distribution of the petal measurements exists because of the different species. This is actually great news for our classification task since it means that the petal measurements will make it easy to distinguish between `Iris-setosa` and the other `Iris` types. Distinguishing `Iris-versicolor` and `Iris-virginica` will prove more difficult given the overlap in their measurements.\n",
    "\n",
    "There are also correlations between petal length and petal width, as well as sepal length and sepal width. The field biologists assure us that this is to be expected: Longer flower petals also tend to be wider, and the same applies for sepals.\n",
    "\n",
    "Now we have a better understanding of the data we are dealing with. Let's finally get to some modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Classification\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "You might be surprised that it took us so long to get to the actual modeling step. In general, data preparation tends to be by far the [most time-consuming](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/) step in data science, and it is a vital one: If we had jumped straight to the modeling, we would have created a faulty classification model. Remember to always check your data first: **Bad data leads to bad models.** \n",
    "\n",
    "Before we select and train a classification model, we need to **split the data into training and testing sets:**\n",
    "- A **training set** is a random subset of the data that we use to train our models.\n",
    "- A **testing set** is a random subset of the data (mutually exclusive from the training set) that we use to validate our models on data they have not seen before.\n",
    "\n",
    "Especially in sparse data sets like ours, it's easy for models to **overfit** the data: The model will learn the training set so well that it won't be able to generalize and handle most of the cases it's never seen before. This is why it's important for us to build the model with the training set, but score it with the testing set.\n",
    "\n",
    "Note that once we split the data into a training and testing set, we should treat the testing set like it no longer exists: We must use any information from the testing set to build our model or else we're cheating.\n",
    "\n",
    "Let's set up our data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using all four measurements as inputs to the model\n",
    "# Note that scikit-learn expects each entry to be a list of values, e.g.,\n",
    "# [ [val1, val2, val3],\n",
    "#   [val1, val2, val3],\n",
    "#   ... ]\n",
    "# such that our input data set is represented as a list of lists\n",
    "\n",
    "# We can extract the data in this format from the DataFrame like this:\n",
    "all_inputs = iris_data_clean[['sepal_length_cm', 'sepal_width_cm',\n",
    "                             'petal_length_cm', 'petal_width_cm']].values\n",
    "# Make sure to not mix up the order of the entries\n",
    "\n",
    "# Similarly, we can extract the class labels\n",
    "all_labels = iris_data_clean['class'].values\n",
    "\n",
    "# Here's what a subset of our inputs looks like:\n",
    "all_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the representation is correct for a random input data point n\n",
    "n = 42\n",
    "\n",
    "print(all_inputs[n],all_labels[n])\n",
    "iris_data_clean.loc[[n]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine! Now our data is ready to be split. We split the data randomly using the method \"train_test_split\" from the **scikit-learn** library, the essential machine learning package in Python. We can specify what fraction of the data should be used for the test set using the parameter `test_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(training_inputs,\n",
    " testing_inputs,\n",
    " training_classes,\n",
    " testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data split, we can start fitting models to our data. We will use a **decision tree classifier**: In their simplest form, decision tree classifiers ask a series of Yes/No questions about the data — each time getting closer to finding out the class of each entry — until they either classify the data set perfectly or simply can't differentiate a set of entries. \n",
    "\n",
    "\n",
    "Note: There are several [parameters](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) that we can tune for decision tree classifiers, but for now we use a basic decision tree classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "\n",
    "# Validate the classifier on the testing set using classification accuracy\n",
    "decision_tree_classifier.score(testing_inputs, testing_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy: Our model achieves **97% classification accuracy** without much effort. However, there's a catch: The split of the data set was done randomly; depending on how our training and testing set is sampled, our model can achieve anywhere from 80% to 100% accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = []\n",
    "\n",
    "# Split data set randomly 1000 times, and train and test the model\n",
    "for repetition in range(1000):\n",
    "    (training_inputs,\n",
    "     testing_inputs,\n",
    "     training_classes,\n",
    "     testing_classes) = train_test_split(all_inputs, all_labels, test_size=0.25)\n",
    "    \n",
    "    decision_tree_classifier = DecisionTreeClassifier()\n",
    "    decision_tree_classifier.fit(training_inputs, training_classes)\n",
    "    classifier_accuracy = decision_tree_classifier.score(testing_inputs, testing_classes)\n",
    "    model_accuracies.append(classifier_accuracy)\n",
    "\n",
    "# Display the accuracy distribution obtained    \n",
    "plt.hist(model_accuracies)\n",
    "plt.xlabel('model accuracy')\n",
    "plt.ylabel('number of repetitions')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obviously a problem that our model performs quite differently depending on the subset of the data it's trained on. A better way to evaluate the model is to use **cross-validation**.\n",
    "\n",
    "## Step 6: Evaluation with cross-validation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "A better approach to evaluate our model is to perform a ***k*-fold cross-validation**, which works as follows: Split the original data set into *k* subsets called *folds*; use one of the folds for evaluation, and train on the rest of the folds. Repeat *k* times such that each fold is used as the testing set exactly once. \n",
    "\n",
    "10-fold cross-validation is the most common choice. We can perform 10-fold cross-validation on our model with the code below. The result is an array containing the 10 evaluation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "k = 10\n",
    "\n",
    "# cross_val_score returns a list of the scores, which we can visualize\n",
    "# to get a reasonable estimate of our classifier's performance\n",
    "cv_scores = cross_val_score(decision_tree_classifier, all_inputs, all_labels, cv=k)\n",
    "\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `cross_val_score` method actually performs a **stratified *k*-fold cross-validation**. Stratified *k*-fold keeps the class proportions the same across all of the folds, which is vital for ensuring that the testing is performed on a representative subset of the data set.\n",
    "\n",
    "Now we have a much more consistent rating of our classifier's general classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cv_scores)\n",
    "plt.title('Average score: {}'.format(np.mean(cv_scores)))\n",
    "plt.xlabel('model accuracy')\n",
    "plt.ylabel('number of folds')\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We finally have our demo classifier in a complete and reproducible machine learning pipeline. We've met the success criteria that we set from the beginning (>90% accuracy), and our pipeline is flexible enough to handle new inputs or flowers when that data set is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>**It's your turn**</font> \n",
    "\n",
    "As a little exercise, let's investigate what result we would obtain for this classifier if we did not correct for the six outliers in the data. Using most of the code shown above, the steps you need to take are:\n",
    "\n",
    "- Load the required libraries and \n",
    "- Import the data, adjust mislabeled classes and missing petal width values\n",
    "- Save a new data set `iris-data-foo` without correcting the outliers \n",
    "- Extract the data into the format required by scikit-learn\n",
    "- Train & evaluate your model using cross-validation \n",
    "- State the average accuracy obtained \n",
    "\n",
    "Before you start, take a moment to reflect: Would you expect the classification accuracy to deteriorate considerably? Why / why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here or create a new notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "Based on a notebook by [Randal S. Olson](http://www.randalolson.com/). \n",
    "\n",
    "Sources for pictures:\n",
    "* iris-types.png: https://gadictos.com/iris-data-classification-using-neural-net/\n",
    "* iris_petal_sepal.png: https://holgerbrandl.github.io/kotlin4ds_kotlin_night_frankfurt/krangl_example_report.html\n",
    "\n",
    "### Python packages used\n",
    "\n",
    "This notebook uses several standard Python packages. These are:\n",
    "\n",
    "* **pandas**: Provides the \"DataFrame\" structure to store data in memory and work with it easily and efficiently. DataFrame is a 2-dimensional labeled data structure with columns of potentially different types; you can think of it like a spreadsheet.\n",
    "* **matplotlib**: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n",
    "* **Seaborn**: Advanced statistical plotting library.\n",
    "* **scikit-learn**: The essential Machine Learning package in Python.\n",
    "* **NumPy**: Provides a fast numerical array structure and helper functions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
